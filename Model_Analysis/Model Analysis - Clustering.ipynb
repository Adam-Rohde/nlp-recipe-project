{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Model Analysis - Clustering.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyP60TFXCRaaxEorzjXw+cCt"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"cXa5xYagBCWT","colab_type":"code","outputId":"a131f19b-1d28-4fd4-ee70-5f6cd21f3bda","executionInfo":{"status":"ok","timestamp":1591114782224,"user_tz":420,"elapsed":2128,"user":{"displayName":"ADAM ROHDE","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh2RG6Buyj0UxBoiReRDtR7J-jCtKa_TESyteCi=s64","userId":"13445948287169092372"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["'''\n","Model Analysis - Clustering\n","'''\n","import os\n","import pickle\n","import re\n","import nltk\n","nltk.download('punkt')\n","import heapq\n","import numpy as np\n","from random import shuffle\n","from sklearn.metrics import pairwise_distances\n","from scipy.spatial.distance import cosine\n","from sklearn.cluster import KMeans\n","import pandas as pd\n","from sklearn.decomposition import PCA\n","import matplotlib.pyplot as plt\n","from sklearn.preprocessing import StandardScaler"],"execution_count":0,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"QN_zeRJNDYPE","colab_type":"code","outputId":"f3d13462-ba61-4564-b6ed-249e56f163c7","executionInfo":{"status":"ok","timestamp":1591114802526,"user_tz":420,"elapsed":17503,"user":{"displayName":"ADAM ROHDE","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh2RG6Buyj0UxBoiReRDtR7J-jCtKa_TESyteCi=s64","userId":"13445948287169092372"}},"colab":{"base_uri":"https://localhost:8080/","height":122}},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"cUY2VQwGJzHd","colab_type":"code","colab":{}},"source":["####################################\n","# LOAD Model output\n","####################################\n","with open('/content/drive/My Drive/CS 263 Project/Final Project Data/BoW_info_full.pickle','rb') as f:\n","    BoW = pickle.load(f)\n","with open('/content/drive/My Drive/CS 263 Project/Final Project Data/bert_similarities.p','rb') as f:\n","    BERT = pickle.load(f)\n","with open('/content/drive/My Drive/CS 263 Project/Final Project Data/doc2vec_similarity.pickle','rb') as f:\n","    D2V = pickle.load(f)\n","with open('/content/drive/My Drive/CS 263 Project/Final Project Data/BiLSTM_similarity.pickle','rb') as f:\n","    LSTM = pickle.load(f)\n","\n","#Embeddings \n","BoW_Embeddings = BoW[\"BoW\"][\"embedding\"]\n","BERT_Embeddings = BERT['embeddings']\n","D2V_Embeddings = D2V[0]\n","LSTM_Embeddings = LSTM[0]\n","\n","#IDs\n","BoW_IDs = BoW[\"BoW\"][\"ids\"]\n","BERT_IDs = BERT['ids']\n","D2V_IDs = D2V[1]\n","LSTM_IDs = LSTM[1]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"C2wwxTYsDhET","colab_type":"code","colab":{}},"source":["####################################\n","# LOAD FULL BAKING DATA\n","####################################\n","#Lang Class\n","class Lang:\n","    def __init__(self, name):\n","        self.name = name\n","        self.word2index = {}\n","        self.word2count = {}\n","        self.index2word = {0: \"SOS\", 1: \"EOS\"}\n","        self.n_words = 2  # Count SOS and EOS\n","    def addSentence(self, sentence):\n","        for word in sentence.split(' '):\n","            self.addWord(word)\n","    def addWord(self, word):\n","        if word not in self.word2index:\n","            self.word2index[word] = self.n_words\n","            self.word2count[word] = 1\n","            self.index2word[self.n_words] = word\n","            self.n_words += 1\n","        else:\n","            self.word2count[word] += 1\n","\n","#get data\n","with open('/content/drive/My Drive/CS 263 Project/Final Project Data/baking_data_title_ingredients.pickle','rb') as f:\n","    baking_data = pickle.load(f)\n","\n","with open('/content/drive/My Drive/CS 263 Project/Final Project Data/nutritional_info.pickle','rb') as f:\n","    nutritional_df = pickle.load(f)\n","\n","# Index out\n","health_mask = baking_data[0].id.isin(nutritional_df.id)\n","\n","# baking dataframe\n","df = baking_data[0][health_mask] #5000 obs\n","list(df.columns)\n","\n","#nutrition df\n","list(nutritional_df.columns)\n","\n","# Recipe IDs\n","baking_ids = df.id.values\n","baking_strings = np.array(baking_data[1])[health_mask].tolist()\n","\n","# list of strings representing each recipe (instructions only)\n","# Remove Separaters\n","baking_strings = [item.replace('--|||--', '').replace('||', '') for item in np.array(baking_data[1])[health_mask].tolist()]\n","# Remove extra white space\n","baking_strings = [\" \".join(item.split()) for item in baking_strings]  \n","            \n","baking_ids_with_strings = list(zip(baking_strings, baking_ids))\n","\n","baking_sent = [instructions[0] for instructions in baking_ids_with_strings]\n","ids_limited= [id[1] for id in baking_ids_with_strings]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"EmMNAk871d-d","colab_type":"code","colab":{}},"source":["####################################\n","# LOAD NUTRITIONAL DATA\n","####################################\n","\n","#get embeddings for recipies with nutritional info\n","with open('/content/drive/My Drive/CS 263 Project/Final Project Data/nutritional_info.pickle','rb') as f:\n","    nutritional_info = pickle.load(f)\n","\n","def nutri_cat(dta,type_text,type_cat):\n","  uni_labels = dta.groupby([type_text]).size().reset_index().rename(columns={0:'count'})\n","  uni_labels[type_cat] = uni_labels.index\n","  dta = pd.merge(dta, uni_labels, on=[type_text], how='left')\n","  return dta[type_cat].to_list()\n","\n","sugars_category = nutri_cat(nutritional_info,'sugars','category_sugars')\n","fat_category = nutri_cat(nutritional_info,'fat','category_fat')\n","salt_category = nutri_cat(nutritional_info,'salt','category_salt')\n","saturates_category = nutri_cat(nutritional_info,'saturates','category_saturates')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"6mJ8QZo3Dall","colab_type":"code","colab":{}},"source":["#######################################################\n","## PCA & K-Means Clustering Functions\n","#######################################################  \n","from sklearn.decomposition import PCA\n","import matplotlib.pyplot as plt\n","from sklearn.cluster import KMeans\n","\n","#PCA on embeddings function\n","def PCA_funk(embdeddings,nPCs=5): \n","  pca = PCA(n_components=nPCs).fit(embdeddings)\n","  return(pca.transform(embdeddings),pca.explained_variance_ratio_)\n","\n","#K-Means clustering on embeddings function\n","def KM_Funk(embeddings,nclus=10):\n","  km = KMeans(n_clusters=nclus)\n","  km.fit(embeddings)\n","  return(km.labels_.tolist())"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"UkVWJgBWYadA","colab_type":"code","colab":{}},"source":["#######################################################\n","## K-Means PCA Plot Functions\n","####################################################### \n","\n","#K-Means PCA Plot Function\n","def KM_PCA_PlotFunk(embeddings,clusters,title,nclus=10,nPCs=5,PCA_Plot=True):\n","  #run PCA on embeddings\n","  pca_trans,var_exp = PCA_funk(embeddings,nPCs)\n","  #plot variance explained by PCs\n","  if PCA_Plot:\n","    plt.bar(range(len(var_exp)), var_exp, color='black')\n","    plt.title('PC Variance Explained for '+title)\n","    plt.ylabel('Variance Explained')\n","    plt.xlabel('Principle Component')\n","    plt.show()\n","  #plot PC1 vs PC2 colored by K-means clustering\n","  scatter = plt.scatter(pca_trans[:,0], pca_trans[:,1], c=clusters,alpha=0.1)\n","  #plt.legend(*scatter.legend_elements(num=nclus),loc=\"upper left\", title=\"Cluster\")\n","  plt.title('K-Means Clustering for '+title)\n","  plt.ylabel('Principle Component 2')\n","  plt.xlabel('Principle Component 1')\n","  if np.max(pca_trans[:,0])>500:\n","    plt.xlim(-500,500)\n","  plt.show()\n","  return(scatter)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"82gF1KniNINl","colab_type":"code","colab":{}},"source":["#######################################################\n","## Most common words in clusters Functions\n","#######################################################  \n","\n","#ClusterWords Function\n","def ClusterWords(clusters,title,num_clusters):\n","\n","  print(\"###########################\")\n","  print(title)\n","  print(\"###########################\")\n","\n","  #create cluster & recipe dataframe\n","  cluster_data = { 'recipe': ids_limited, 'cluster': clusters}\n","  frame = pd.DataFrame(cluster_data, index = [clusters] , columns = ['recipe', 'cluster'])\n","  frame['string'] = baking_strings\n","  frame['cluster'].value_counts()\n","\n","  #print first 10 recipes in each cluster\n","  for i in range(num_clusters):\n","      recipe_ids = frame.recipe[i]\n","      print(\"Cluster %d Recipes:\" % i)\n","      print(df[df.id.isin(recipe_ids)].title.values[0:10])\n","\n","  ## Find keyword ingredients\n","  # CLUSTER 0\n","  wordfreq_cluster0 = {} \n","  cluster0_mask = frame['cluster']==0\n","  cluster0_df = frame[cluster0_mask]\n","  cluster0_baking_sent = cluster0_df['string'].tolist()\n","  for sentence in cluster0_baking_sent:\n","      tokens = nltk.word_tokenize(sentence)\n","      for token in tokens:\n","          if token not in wordfreq_cluster0.keys():\n","              wordfreq_cluster0[token] = 1\n","          else:\n","              wordfreq_cluster0[token] += 1\n","  most_freq_cluster0 = heapq.nlargest(100, wordfreq_cluster0, key=wordfreq_cluster0.get)            \n","  print(most_freq_cluster0)  \n","\n","  # CLUSTER 1\n","  wordfreq_cluster1 = {} \n","  cluster1_mask = frame['cluster']==1\n","  cluster1_df = frame[cluster1_mask]\n","  cluster1_baking_sent = cluster1_df['string'].tolist()\n","  for sentence in cluster1_baking_sent:\n","      tokens = nltk.word_tokenize(sentence)\n","      for token in tokens:\n","          if token not in wordfreq_cluster1.keys():\n","              wordfreq_cluster1[token] = 1\n","          else:\n","              wordfreq_cluster1[token] += 1\n","  most_freq_cluster1 = heapq.nlargest(100, wordfreq_cluster1, key=wordfreq_cluster1.get)                    \n","  print(most_freq_cluster1)\n","\n","  #find unique words in each cluster\n","  unique0 = set(most_freq_cluster0) - set(most_freq_cluster1)\n","  print(unique0)\n","  unique1 = set(most_freq_cluster1) - set(most_freq_cluster0)\n","  print(unique1)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"g5U0IuS6Vl_q","colab_type":"code","colab":{}},"source":["#######################################################\n","## Run Cluster Analyses \n","#######################################################  \n","\n","#set number of clusters and PCs\n","num_pcs = 5\n","num_clusters=2"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"rhW6HBCAVEzB","colab_type":"code","colab":{}},"source":["#Get Clusters\n","D2V_Clusters = KM_Funk(D2V_Embeddings,nclus=num_clusters)\n","BERT_Clusters = KM_Funk(BERT_Embeddings,nclus=num_clusters)\n","LSTM_Clusters = KM_Funk(LSTM_Embeddings,nclus=num_clusters)\n","BoW_Clusters = KM_Funk(BoW_Embeddings,nclus=num_clusters)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"AOT2Qyi_NO2F","colab_type":"code","colab":{}},"source":["#Cluster PCA Plots\n","KM_PCA_PlotFunk(D2V_Embeddings,D2V_Clusters,\"D2V Embeddings\",nclus=num_clusters,nPCs=num_pcs,PCA_Plot=True)\n","KM_PCA_PlotFunk(BERT_Embeddings,BERT_Clusters,\"BERT Embeddings\",nclus=num_clusters,nPCs=num_pcs,PCA_Plot=True)\n","KM_PCA_PlotFunk(LSTM_Embeddings,LSTM_Clusters,\"LSTM Embeddings\",nclus=num_clusters,nPCs=num_pcs,PCA_Plot=True)\n","KM_PCA_PlotFunk(BoW_Embeddings,BoW_Clusters,\"BoW Embeddings\",nclus=num_clusters,nPCs=num_pcs,PCA_Plot=True)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"oRsSuJTh3HLf","colab_type":"code","colab":{}},"source":["#Nutritional Category PCA Plots\n","d2v_sugar = KM_PCA_PlotFunk(D2V_Embeddings,sugars_category,\"D2V Embeddings - Sugar Levels\",nclus=3,nPCs=num_pcs,PCA_Plot=False)\n","KM_PCA_PlotFunk(BERT_Embeddings,sugars_category,\"BERT Embeddings - Sugar Levels\",nclus=3,nPCs=num_pcs,PCA_Plot=False)\n","KM_PCA_PlotFunk(LSTM_Embeddings,sugars_category,\"LSTM Embeddings - Sugar Levels\",nclus=3,nPCs=num_pcs,PCA_Plot=False)\n","KM_PCA_PlotFunk(BoW_Embeddings,sugars_category,\"BoW Embeddings - Sugar Levels\",nclus=3,nPCs=num_pcs,PCA_Plot=False)\n","\n","KM_PCA_PlotFunk(D2V_Embeddings,fat_category,\"D2V Embeddings - Fats Levels\",nclus=3,nPCs=num_pcs,PCA_Plot=False)\n","KM_PCA_PlotFunk(BERT_Embeddings,fat_category,\"BERT Embeddings - Fats Levels\",nclus=3,nPCs=num_pcs,PCA_Plot=False)\n","KM_PCA_PlotFunk(LSTM_Embeddings,fat_category,\"LSTM Embeddings - Fats Levels\",nclus=3,nPCs=num_pcs,PCA_Plot=False)\n","KM_PCA_PlotFunk(BoW_Embeddings,fat_category,\"BoW Embeddings - Fats Levels\",nclus=3,nPCs=num_pcs,PCA_Plot=False)\n","\n","KM_PCA_PlotFunk(D2V_Embeddings,salt_category,\"D2V Embeddings - Salt Levels\",nclus=3,nPCs=num_pcs,PCA_Plot=False)\n","KM_PCA_PlotFunk(BERT_Embeddings,salt_category,\"BERT Embeddings - Salt Levels\",nclus=3,nPCs=num_pcs,PCA_Plot=False)\n","KM_PCA_PlotFunk(LSTM_Embeddings,salt_category,\"LSTM Embeddings - Salt Levels\",nclus=3,nPCs=num_pcs,PCA_Plot=False)\n","KM_PCA_PlotFunk(BoW_Embeddings,salt_category,\"BoW Embeddings - Salt Levels\",nclus=3,nPCs=num_pcs,PCA_Plot=False)\n","\n","KM_PCA_PlotFunk(D2V_Embeddings,saturates_category,\"D2V Embeddings - Saturates Levels\",nclus=3,nPCs=num_pcs,PCA_Plot=False)\n","KM_PCA_PlotFunk(BERT_Embeddings,saturates_category,\"BERT Embeddings - Saturates Levels\",nclus=3,nPCs=num_pcs,PCA_Plot=False)\n","KM_PCA_PlotFunk(LSTM_Embeddings,saturates_category,\"LSTM Embeddings - Saturates Levels\",nclus=3,nPCs=num_pcs,PCA_Plot=False)\n","KM_PCA_PlotFunk(BoW_Embeddings,saturates_category,\"BoW Embeddings - Saturates Levels\",nclus=3,nPCs=num_pcs,PCA_Plot=False)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"S3OGfQhcZMNd","colab_type":"code","colab":{}},"source":["#ClusterWords\n","ClusterWords(D2V_Clusters,\"D2V\",num_clusters)\n","ClusterWords(BERT_Clusters,\"BERT\",num_clusters)\n","ClusterWords(LSTM_Clusters,\"LSTM\",num_clusters)\n","ClusterWords(BoW_Clusters,\"BoW\",num_clusters)"],"execution_count":0,"outputs":[]}]}
